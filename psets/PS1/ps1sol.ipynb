{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center\">18.335/6.337 Problem Set 1 - Floating Point and Norms</div>\n",
    "## <div style=\"text-align: center\">Solution</div>\n",
    "### <div style=\"text-align: center\">Created by Wonseok Shin</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\Cmat}[2]{\\mathbb{C}^{#1\\times#2}}\\newcommand{\\Cvec}[1]{\\mathbb{C}^{#1}}\\newcommand{\\Rmat}[2]{\\mathbb{R}^{#1\\times#2}}\\newcommand{\\Rvec}[1]{\\mathbb{R}^{#1}}\\newcommand{\\null}{\\mathrm{null}}\\newcommand{\\range}{\\mathrm{range}}\\newcommand{\\rank}{\\mathrm{rank}}\\newcommand{\\nullity}{\\mathrm{nullity}}\\newcommand{\\sign}{\\mathrm{sign}}\\newcommand{\\norm}[1]{\\left\\|#1\\right\\|}\\newcommand{\\abs}[1]{\\left|#1\\right|}\\newcommand{\\epsmach}{\\epsilon_\\mathrm{machine}}\\newcommand{\\log}{\\mathrm{log}}\\newcommand{\\tanh}{\\mathrm{tanh}}\\newcommand{\\l}{\\lambda}\\newcommand{\\d}{\\delta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.  Floating-point peculiarity [4 pts]\n",
    "\n",
    "In this problem, we will see some peculiar behaviors of floating-point numbers and try to understand them.\n",
    "\n",
    "A few things to keep in mind:\n",
    "\n",
    "- As discussed in class, the bit representation of IEEE double-precision floating point numbers is $S\\,E_1 \\cdots E_{11} M_1 \\cdots M_{52}$, where\n",
    "    - a single bit $S$ stores the sign,\n",
    "    - 11 bits $E_1 \\cdots E_{11}$ store the exponent information, and \n",
    "    - 52 bits $M_1 \\cdots M_{52}$ store the mantissa (or significand) information.\n",
    "- `eps()` in Julia returns the gap between 1 and the next larger number in the double-precision floating-point system.  \n",
    "    - This is *not* the \"machine epsilon\" $\\epsmach = \\frac{1}{2}\\beta^{-(t-1)}$ defined in Eq. (13.3) of T&B, but is twice as big, i.e., `eps()` $=2\\epsmach$.\n",
    "    - T&B's definition of machine epsilon is not universal: many other authors take the value of `eps()` itself (not a half) as machine epsilon, as shown [here](https://en.wikipedia.org/wiki/Machine_epsilon#Values_for_standard_hardware_floating_point_arithmetics).\n",
    "- `bin()` in Julia is useful for obtaining the binary representation of integers.  For example, `bin(23)` returns `\"10111\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) [0.5 pts] Find the bit representation of the value returned by `eps()`.  (Indicate the values of the sign bit $S$, exponent bits $E_1 \\cdots E_{11}$, and the mantissa bits $M_1 \\cdots M_{52}$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution.***\n",
    "\n",
    "`eps()` is $(-1)^0 \\times 1.0 \\cdots 0_{(2)} \\times 2^{-52} = (-1)^s m \\beta^e$.  Therefore, \n",
    "- $S = 0$, \n",
    "- $M_1 \\cdots M_{52} = 0 \\cdots 0$, and \n",
    "- $E_1 \\cdots E_{11}$ is the binary representation of $-52 + 1023$, which is $01111001011$ as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1111001011\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(-52+1023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the complete bit representation for `eps()` is $SE_1 \\cdots E_{11}M_1 \\cdots M_{52} = 0011110010110000000000000000000000000000000000000000000000000000$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) [1 pt] Evaluate `1+eps()-1` and `2+eps()-2`.  Explain why you get different results.  (Hint: fundamental axiom of floating-point arithmetic.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution.***\n",
    "\n",
    "The operations used in these evaluations is a floating point operation.  According to the fundamental axiom of floating-point arithmetic,\n",
    "\n",
    "`1+eps()` = $1 \\oplus 2\\epsmach = fl(1+2\\epsmach) = fl({1.\\overbrace{0 \\cdots 0 1}^{\\text{52 bits}}}_{(2)} \\times 2^0) = {1.\\overbrace{0 \\cdots 0 1}^{\\text{52 bits}}}_{(2)} \\times 2^0 = 1 + 2\\epsmach$\n",
    "\n",
    "and\n",
    "\n",
    "`2+eps()` = $2 \\oplus 2\\epsmach = fl(2+2\\epsmach) = fl(2(1+\\epsmach)) = fl(2 \\times ({1.\\overbrace{0 \\cdots 0 1}^{\\text{53 bits}}}_{(2)} \\times 2^0)) = fl({1.\\overbrace{0 \\cdots 0 1}^{\\text{53 bits}}}_{(2)} \\times 2^1) = {1.\\overbrace{0 \\cdots 0}^{\\text{52 bits}}}_{(2)} \\times 2^1 = 2$.\n",
    "\n",
    "In other words, `1+eps()` will remain exact whereas `2+eps()` will be approximated by 2.  Therefore `1+eps()-1` returns `eps()` and `2+eps()-2` returs 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) [0.5 pts] Evaluate `(2^-537.)^2` and `(2^-538.)^2`.  Explain why you get 0 from the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution.***\n",
    "\n",
    "In exact arithmetic, `(2^-537.)^2` and `(2^-538.)^2` are $2^{-1074}$ and $2^{-1076}$, respectively.  The former is the smallest denormalized number of the double-precision floating-point format, whereas the latter is smaller than that.  Therefore, the latter suffers from underflow and is approximated by 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) [1 pt] (Taken from Exercise 13.2b of T&B)  The double-precision floating-point system includes many integers, but not all of them.  What is the smallest positive integer $n$ that does not belong to this system?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution.***\n",
    "\n",
    "The double-precision floating-point system has 52 mantissa bits below the radix point.  The positive integers that do not belong to the system are therefore the ones that require more than the 52 mantissa bits.  The smallest such mantissa is ${1.\\overbrace{0\\cdots 01}^{\\text{53 bits}}}_{(2)}$, and $2^{53}$ is the smallest power of two to multiply to this mantissa in order to obtain an integer.  Therefore, the number we are looking for is ${1.\\overbrace{0\\cdots 01}^{\\text{53 bits}}}_{(2)} \\times 2^{53} = 2^{53} + 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) [1 pt] (Taken from Exercise 13.2c of T&B)  For the answer $n$ of Part (d), detertmine whether or not each of $n+1$, $n+2$, $n+3$ belongs to the double-precision floating-point system.\n",
    "\n",
    "Write Julia code that validates your analysis.  Write the code such that it also confirms $n$ indeed does not belong to the double-precision floating-point system, whereas $n-3$, $n-2$, $n-1$ do (as $n$ is the smallest that does not).  (Hint: `float(n)` produces the floating-point approximation of an integer `n`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution.***\n",
    "\n",
    "For $n = 2^{53} + 1 = {1\\overbrace{0\\cdots 01}^{\\text{53 bits}}}_{(2)} = {1.\\overbrace{0\\cdots 01}^{\\text{53 bits}}}_{(2)}\\times 2^{53}$,\n",
    "\n",
    "- $n+1 = {1\\overbrace{0\\cdots 10}^{\\text{53 bits}}}_{(2)} = {1.\\overbrace{0\\cdots 01}^{\\text{52 bits}}}_{(2)} \\times 2^{53}$ requires 52 mantissa bits,\n",
    "- $n+2 = {1\\overbrace{0\\cdots 11}^{\\text{53 bits}}}_{(2)} = {1.\\overbrace{0\\cdots 11}^{\\text{53 bits}}}_{(2)} \\times 2^{53}$ requires 53 mantissa bits, and\n",
    "- $n+3 = {1\\overbrace{0\\cdots 100}^{\\text{53 bits}}}_{(2)} = {1.\\overbrace{0\\cdots 01}^{\\text{51 bits}}}_{(2)} \\times 2^{53}$ requires 51 mantissa bits.\n",
    "\n",
    "Therefore, $n+1$ and $n+3$ fit within the 52 mantissa bits of the IEEE double-precision floating-point format, but $n+2$ does not.\n",
    "\n",
    "The following code validates this analysis, and also confirms that $n-3$, $n-2$, $n-1$ indeed belong to the double-precision floating-point system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-3 == float(n-3)?  true\n",
      "n-2 == float(n-2)?  true\n",
      "n-1 == float(n-1)?  true\n",
      "  n == float(n)?    false\n",
      "n+1 == float(n+1)?  true\n",
      "n+2 == float(n+2)?  false\n",
      "n+3 == float(n+3)?  true\n"
     ]
    }
   ],
   "source": [
    "function test_n(n)\n",
    "    println(\"n-3 == float(n-3)?  $(n-3==float(n-3))\")\n",
    "    println(\"n-2 == float(n-2)?  $(n-2==float(n-2))\")\n",
    "    println(\"n-1 == float(n-1)?  $(n-1==float(n-1))\")    \n",
    "    println(\"  n == float(n)?    $(n==float(n))\")\n",
    "    println(\"n+1 == float(n+1)?  $(n+1==float(n+1))\")\n",
    "    println(\"n+2 == float(n+2)?  $(n+2==float(n+2))\")\n",
    "    println(\"n+3 == float(n+3)?  $(n+3==float(n+3))\")\n",
    "end\n",
    "\n",
    "test_n(2^53+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.  Accurate evaluation of mathematical functions [3 pts]\n",
    "\n",
    "In this problem, we will devise ways to evaluate some mathematical functions on computers accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) [1 pt] Evaluation of $f(x) = e^x - 1$ around $x = 0$.\n",
    "\n",
    "- Explain the numerical problem you may experience during this evaluation.\n",
    "- Devise a method to avoid this problem.  (Hint: $\\tanh(x)$.)\n",
    "- Implement your method in `f_new(x)` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution.***\n",
    "\n",
    "For $x \\approx 0$, $e^x \\approx 1$, so subtracting $1$ from $e^x$ suffers from catastrophic cancellation.\n",
    "\n",
    "To avoid this problem, consider $\\tanh(x) = (e^x - e^{-x}) / (e^x + e^{-x}) = (e^{2x}-1) / (e^{2x}+1)$.  From this we can write $\\tanh(x/2) = (e^x-1) / (e^x+1)$, which gives the following equivalent expression for $e^x-1$:\n",
    "\n",
    "$$\n",
    "e^x-1 = \\tanh\\left(\\frac{x}{2}\\right) (e^x + 1).\n",
    "$$\n",
    "\n",
    "The right-hand side avoids catastrophic cancellation for $x \\approx 0$, so it produces accurate results even for small x.\n",
    "\n",
    "Note: because $e^x - 1$ shows up quite often during computation, Julia has a dedicated function `expm1(x)` (\"exponential minus 1\"), whose specific implementation could be different from the one shown in this solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f_new (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_old(x) = exp(x) - 1\n",
    "f_new(x) = tanh(x/2) * (exp(x)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of your implementation\n",
    "\n",
    "Validate your implementation of `f_new(x)` by using the following code block that compares the numerical derivatives of `f_old(x)` and `f_new(x)` at `x = 0`.  Unlike `f_old(x)` that produces a completely wrong derivative 0.0, `f_new(x)` must produce a derivative close to the exact value 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_old'(0) =  0.0\n",
      "f_new'(0) =  1.0\n"
     ]
    }
   ],
   "source": [
    "x = 0.0\n",
    "dx = eps() / 10\n",
    "println(\"f_old'(0) =  $((f_old(x+dx) - f_old(x)) / dx)\")\n",
    "println(\"f_new'(0) =  $((f_new(x+dx) - f_new(x)) / dx)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) [1 pt] Evaluation of $g(x) = \\log(1+x)$ around $x = 0$.  ($\\log$ is the natural logarithm.)\n",
    "\n",
    "- Explain the numerical problem you may experience during this evaluation.  (Hint: Prob. 1(b).)\n",
    "- Devise a method to avoid this problem.  (Hint: what is the relationship between $e^x-1$ and $\\log(1+x)$?)\n",
    "- Implement your method in `g_new(x)` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution.***\n",
    "\n",
    "When $x$ is much smaller than 1 in magnitude, many mantissa bits are truncated during the evaluation of $1+x$, and thus $\\log(1+x)$ is evaluated inaccurately.  \n",
    "\n",
    "To avoid this problem, note that $e^x - 1$ in Part (a) is the inverse of $\\log(1+x)$.  We should be able to find an accurate expression for $y = \\log(1+x)$ by inverting the accurate expression for $x = e^y-1$ already found in Part (a) as follows:\n",
    "$$\n",
    "x = e^y - 1 = \\tanh\\left(\\frac{y}{2}\\right) (e^y + 1) \\Longleftrightarrow x = \\tanh\\left(\\frac{y}{2}\\right) (x+2) \\Longleftrightarrow y = 2\\tanh^{-1}\\left(\\frac{x}{x+2}\\right)\n",
    "$$\n",
    "for $x>-1$.  Therefore, we have \n",
    "$$\n",
    "\\log(1+x) = 2\\tanh^{-1}\\left(\\frac{x}{x+2}\\right).\n",
    "$$\n",
    "\n",
    "Unlike the left-hand side's argument $1+x$ that is truncated towards a constant value $1$ for small $x$, the right-hand side's argument $\\frac{x}{x+2} = x \\cdot \\frac{1}{x+2}$ has the small $x$ factored out, so its approximation $\\frac{x}{x+2} \\approx \\frac{x}{2}$ for small x still captures variation of small $x$.  Therefore, the right-hand side expression produces accurate results even for small x.\n",
    "\n",
    "Note: because $\\log(1+x)$ shows up quite often during computation, Julia has a dedicated function `log1p(x)` (\"log 1 plus\"), whose specific implementation could be different from the one shown in this solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g_new (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_old(x) = log(1+x)\n",
    "g_new(x) = 2atanh(x/(x+2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of your implementation\n",
    "\n",
    "Validate your implementation of `g_new(x)` by using the following code block that compares the numerical derivatives of `g_old(x)` and `g_new(x)` at `x = 0`.  Unlike `g_old(x)` that produces a completely wrong derivative 0.0, `g_new(x)` must produce a derivative close to the exact value 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_old'(0) =  0.0\n",
      "g_new'(0) =  1.0\n"
     ]
    }
   ],
   "source": [
    "x = 0.0\n",
    "dx = eps() / 10\n",
    "println(\"g_old'(0) =  $((g_old(x+dx) - g_old(x)) / dx)\")\n",
    "println(\"g_new'(0) =  $((g_new(x+dx) - g_new(x)) / dx)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) [1 pt] Evaluation of $h(x,y) = \\sqrt{x^2+y^2}$.\n",
    "\n",
    "- Explain at least one numerical problem that is different from those occurred in Parts (a) and (b) that you may experience during this evaluation.\n",
    "- Devise a method to avoid this problem.\n",
    "- Implement your method in `h_new(x,y)` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution.***\n",
    "\n",
    "When $x$ or $y$ is sufficiently large, $x^2+y^2$ can suffer from overflow and be evaluated to `Inf`.  Then $\\sqrt{x^2+y^2}$ will be evaluated to `Inf` as well, because `sqrt{Inf} = Inf`.  This is undesirable, because in exact arithmetic, the end result $\\sqrt{x^2+y^2}$ will have magnitude closer to that of $x$ or $y$ and therefore should be storable in the double-precision floating-point format.\n",
    "\n",
    "To avoid this problem, we can factor the larger of $x$ and $y$ out of the square root.  For example, if $x > y$, we can transform the equation as\n",
    "$$\n",
    "\\sqrt{x^2+y^2} = \\abs{x} \\sqrt{1+(y/x)^2},\n",
    "$$\n",
    "and evaluating the transformed equation will not suffer from overflow anymore because $y/x$ has magnitude less than $1$.\n",
    "\n",
    "Underflow is another situation we have to worry about.  When *both* $x$ and $y$ are too small in magnitude, the direct evaluation of $x^2$ and $y^2$ may suffer from underflow.  This underflow can be avoided similarly by factoring *either* $x$ or $y$ out of the square root, but still you must factor out the greater of the two.  You might be tempted to factor out the smaller of the two, say $x$ when $\\abs{x}<\\abs{y}$, because then $(y/x)^2$ is greater than $1$ and less likesly to suffer from underflow.  However this route should not be taken because it increases the chance of overflow in $(y/x)^2$.  In fact, it is perfectly fine that $(y/x)^2$ suffers from underflow, because in that case the exact value of $\\sqrt{x^2+y^2}$ cannot be stored accurately within the capacity of double precision anyway.\n",
    "\n",
    "Solutions will receive the full mark if they correctly identify the possibility of either overflow or underflow (or both), even though the validation code block below was created with only the overflow situation in mind.  Points will not be deducted even if you do not take care of the possibility of division by zero occurring for $x = y = 0$.\n",
    "\n",
    "Note: because $\\sqrt{x^2+y^2}$ shows up frequently during computation, Julia has a dedicated function `hypot(x,y)` that avoids overflow and underflow mentioned here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h_new (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_old(x,y) = sqrt(x^2+y^2)\n",
    "h_new(x,y) = abs(x)>abs(y) ? abs(x)*sqrt(1+(y/x)^2) : (y==0.0 ? 0.0 : abs(y)*sqrt(1+(x/y)^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of your implementation\n",
    "\n",
    "Validate your implementation of `h_new(x,y)` by using the following code block that compares the values of `h_old(x,y)` and `h_new(x,y)`.  The value of `h_old(x,y)` does not make sense here, but the value of your `h_new(x,y)` must."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_old(x,y) = Inf\n",
      "h_new(x,y) = 1.414213562373095e200\n"
     ]
    }
   ],
   "source": [
    "x = 1e200\n",
    "y = 1e200\n",
    "println(\"h_old(x,y) = $(h_old(x,y))\")\n",
    "println(\"h_new(x,y) = $(h_new(x,y))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.  Dual norm and $B$ such that $\\left\\| A B x\\right\\| =\\left\\| A\\right\\|  \\left\\| B\\right\\|  \\left\\| x\\right\\|$ [3 pts]\n",
    "\n",
    "(Partly taken from Exercise 3.6 of Trefethen and Bau)\n",
    "\n",
    "In this problem, we will prove step-by-step that for a given square matrix $A \\in \\mathbb{C}^{m\\times m}$ and column vector $x \\in \\mathbb{C}^m$, there exists a square matrix $B$ such that $\\left\\| A B x\\right\\| =\\left\\| A\\right\\|  \\left\\| B\\right\\|  \\left\\| x\\right\\|$, where the matrix norm is induced by the vector norm.  (This fact will be used in proving the important Theorem 12.2 of Trefethen & Bau later in the class.)\n",
    "\n",
    "The *dual norm* turns out to be a useful tool for the proof.  Let $\\left\\| \\cdot \\right\\|$ be any vector norm on $\\mathbb{C}^m$ with $m \\ge 2$.  The dual norm $\\left\\| \\cdot \\right\\|_*$ of the norm $\\left\\| \\cdot \\right\\|$ is defined as\n",
    "\n",
    "$$\n",
    "\\left\\| x\\right\\|_* = \\max_{v \\neq 0} \\frac{\\left| x^* v\\right|}{\\left\\| v \\right\\|} = \\max_{\\left\\| v \\right\\|=1} \\left| x^* v \\right|.\n",
    "$$\n",
    "\n",
    "Note that the dual norm is defined in terms of $\\left\\| \\cdot \\right\\|$.\n",
    "\n",
    "Below, the proofs of some parts rely on the proofs of the previous parts.  If some parts are too difficult to prove, you are welcome to take those parts for granted without proof and use them in proving the next parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) [0.5 pts] Prove that $\\left\\| \\cdot \\right\\|_*$ is indeed a norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Proof.***\n",
    "\n",
    "We need to show that $\\left\\| \\cdot \\right\\|_*$ satisfies the three conditions:\n",
    "1. *Nonnegativity*.  In the above definition $\\left\\| x \\right\\|_*$, $\\left| x^* v \\right|$ is always nonnegative, so $\\left\\| x\\right\\|_* \\ge 0$.  We also need to show $\\left\\| x\\right\\|_* = 0$ iff $x = 0$, and it is obvious that $\\left\\| x\\right\\|_* = 0$ if $x = 0$.  On the other hand, if $x \\neq 0$, then \n",
    "$$\n",
    "\\begin{align}\n",
    "\\left\\| x \\right\\|_* &= \\max_{v\\neq 0} \\frac{\\left| x^* v \\right|}{\\left\\| v\\right\\|}\\\\\n",
    "&\\ge \\frac{\\left| x^* x \\right|}{\\left\\| x\\right\\|} = \\frac{\\left\\| x \\right\\|_2^2}{\\left\\| x \\right\\|} > 0,\n",
    "\\end{align}\n",
    "$$\n",
    "because $\\left\\| x\\right\\|_2 > 0$ and $\\left\\| x \\right\\| > 0$ for $x \\neq 0$.  This means $\\left\\| x \\right\\|_* = 0$ only if $x = 0$.\n",
    "\n",
    "2. *Absolute scalability*.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\| \\alpha  x \\right\\|_* &= \\max_{\\left\\| v \\right\\| =1} \\left| (\\alpha x)^* v \\right|\\\\\n",
    "&= \\max_{\\left\\| v \\right\\| =1} \\left| \\alpha \\right| \\left| x^* v \\right|\\\\\n",
    "&= \\left| \\alpha \\right| \\max_{\\left\\| v \\right\\| =1} \\left| x^* v \\right| = \\left| \\alpha \\right| \\left\\| x \\right\\|_*.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "3. *Triangle inequality*.  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\| x+y \\right\\|_* &= \\max_{\\left\\| v \\right\\| =1} \\left| (x+y)^* v \\right|\\\\\n",
    "&\\le \\max_{\\left\\| v \\right\\| =1} (\\left| x^* v \\right| + \\left| y^* v \\right|)\\\\\n",
    "&\\le \\max_{\\left\\| v \\right\\| =1} \\left| x^* v \\right| + \\max_{\\left\\| v \\right\\| =1}\\left| y^* v \\right| = \\left\\| x \\right\\|_* + \\left\\| y \\right\\|_*.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) [0.5 pts] Prove that $\\left\\| x\\right\\|_{**} \\leq \\left\\| x\\right\\|$ for all $x\\in \\mathbb{C}^m$, i.e., the dual norm of the dual norm is less than or equal to the original norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Proof.***\n",
    "\n",
    "For $x = 0$, the inequality holds because $\\left\\| x\\right\\|_{**} = \\left\\| x \\right\\| = 0$.\n",
    "\n",
    "For $x \\neq 0$, we want to show that $\\left\\| x \\right\\|$ is an upper bound of $\\left\\| x \\right\\|_{**} = \\max_{y \\neq 0} \\frac{\\left| x^* y \\right| }{\\left\\| y \\right\\|_*}$.  The last expression contains the dual norm $\\left\\| y \\right\\|_*$ in the denominator, and from the definition of the dual norm we know $\\left\\| y \\right\\|_* \\ge \\frac{\\left| y^* x \\right| }{\\left\\| x\\right\\| }$ for arbitrary $y$ for the given $x \\neq 0$.  This implies $\\left\\| x \\right\\| \\ge \\frac{\\left| x^* y \\right| }{\\left\\| y\\right\\|_*}$ for arbitrary $y \\neq 0$.  Using this in the definition of $\\left\\| x \\right\\|_{**}$, we get\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left\\| x \\right\\|_{**} &= \\max_{y \\neq 0} \\frac{\\left| x^* y \\right| }{\\left\\| y \\right\\|_*}\\\\\n",
    "&\\le \\max_{y \\neq 0} \\left\\| x \\right\\| = \\left\\| x \\right\\|.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) [0.5 pts] In fact, we have not only $\\left\\| x\\right\\|_{**} \\leq \\left\\| x\\right\\|$ but also $\\left\\| x\\right\\|_{**} = \\left\\| x\\right\\|$ for all $x\\in \\mathbb{C}^m$, i.e., the dual norm of the dual norm *is* the original norm.  Using this fact without proof, show that for a given $x\\in \\mathbb{C}^m$, there exists a nonzero $w\\in \\mathbb{C}^m$ such that $w^* x = \\left\\| w\\right\\|_* \\left\\| x\\right\\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Proof.***\n",
    "\n",
    "For $x = 0$, the equation holds for any $w$.\n",
    "\n",
    "For $x \\neq 0$, consider the relationship $\\left\\| x \\right\\| = \\left\\| x \\right\\|_{**} = \\max_{y \\neq 0} \\frac{\\left| x^* y \\right| }{\\left\\| y \\right\\|_*}$.  This means there exists some $u \\neq 0$ such that $\\left\\| x \\right\\| = \\frac{\\left| x^* u \\right| }{\\left\\| u \\right\\|_*}$, or equivalently \n",
    "$$\n",
    "\\left| u^* x \\right| = \\left\\| u \\right\\|_* \\left\\| x \\right\\|.\n",
    "$$\n",
    "\n",
    "Notice $w = u$ almost satisfies the desired equation for $w$, except that $\\left| u^* x \\right|$ may not be $u^* x$.  However, even if $u$ is not the desired $w$, we can multiply a phase factor $e^{i\\theta}$ to $u$ in order to cancel the nonzero phase angle of $u^* x$ and make $(e^{i\\theta} u)^* x$ real and positive without changing the right-hand side of the above equation.  For such phase factor $e^{i\\theta}$, we have\n",
    "$$\n",
    "(e^{i\\theta} u)^* x = \\left| (e^{i\\theta} u)^* x \\right| = \\left| u^* x \\right| = \\left\\| u \\right\\|_* \\left\\| x \\right\\| = \\left\\| e^{i\\theta} u \\right\\|_* \\left\\| x \\right\\|,\n",
    "$$\n",
    "which means $e^{i\\theta} u$ is the desired $w$.\n",
    "\n",
    "Because $e^{i\\theta}$ is chosen to cancel the phase angle in $(e^{i\\theta} u)^* x = e^{-i\\theta} (u^* x)$,  it must have the same phase angle as $u^* x$.  Therefore, $e^{i\\theta} = \\frac{u^* x}{\\left| u^* x \\right|}$, and $$\n",
    "w = e^{i\\theta} u = \\frac{u^* x}{\\left| u^* x \\right|} u.\n",
    "$$\n",
    "You can easily check this $w$ indeed satisfies $w^* x = \\left\\| w\\right\\|_* \\left\\| x\\right\\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) [1 pt] Let nonzero $x,y\\in \\mathbb{C}^m$ be given.  Show that there exists a rank-1 matrix $B = y z^*$ such that $B x = y$ and $\\left\\| B\\right\\|  \\left\\| x\\right\\| =\\left\\| y\\right\\|$, where $\\left\\| B\\right\\|$ is the matrix norm of $B$ induced by the vector norm $\\left\\| \\cdot \\right\\|$.  (Hint: use Part (c).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Proof.***\n",
    "\n",
    "The goal is to find $z$ satisfying the conditions.  The condition $B x = y (z^* x) = y$ for nonzero $y$ implies $z^* x = 1$.  Also, $\\left\\| B \\right\\|$ is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left\\| B \\right\\| &= \\max_{\\left\\| v \\right\\| = 1} \\left\\| B v \\right\\|\\\\\n",
    "&= \\max_{\\left\\| v \\right\\| = 1} \\left\\| y (z^* v) \\right\\|\\\\\n",
    "&= \\max_{\\left\\| v \\right\\| = 1} \\left| z^* v \\right| \\left\\| y \\right\\|\\\\\n",
    "&= \\left\\| y \\right\\| \\max_{\\left\\| v \\right\\| = 1} \\left| z^* v \\right| = \\left\\| y \\right\\| \\left\\| z \\right\\|_*.\n",
    "\\end{align}\n",
    "$$\n",
    "Therefore, the condition $\\left\\| B\\right\\|  \\left\\| x\\right\\| =\\left\\| y\\right\\|$ implies $\\left\\| z \\right\\|_* \\left\\| x \\right\\| = 1$.\n",
    "\n",
    "So, we need to find $z$ such that $z^* x = \\left\\| z \\right\\|_* \\left\\| x \\right\\| = 1$.  In Part (c), we have found $w$ such that $w^* x = \\left\\| w\\right\\|_* \\left\\| x\\right\\|$ for a given $x$.  By comparison, we can readily construct $z$ by scaling $w$ down by a factor of $\\left\\| w\\right\\|_* \\left\\| x\\right\\|$, namely $z = \\frac{w}{\\left\\| w\\right\\|_* \\left\\| x\\right\\|}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) [0.5 pts] For a given square matrix $A\\in \\mathbb{C}^{m\\times m}$ and column vector $x\\in \\mathbb{C}^m$, show that there exists $B\\in \\mathbb{C}^{m\\times m}$ such that $\\left\\| A B x \\right\\| =\\left\\| A \\right\\| \\left\\| B\\right\\| \\left\\| x\\right\\|$, where the matrix norm is induced by the vector norm.  (Hint: use Part (d).)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Proof.***\n",
    "\n",
    "For $x = 0$, any $B$ satisfies the equation.\n",
    "\n",
    "For $x \\neq 0$, note first that there exists a nonzero $y\\in \\mathbb{C}^m$ such that $\\left\\| A y \\right\\| = \\left\\| A \\right\\| \\left\\| y \\right\\|$ because $\\left\\| A \\right\\| = \\max_{v \\neq 0} \\frac{\\left\\| A v \\right\\|}{\\left\\| v \\right\\|}$.  For such $y \\neq 0$ and the given $x \\neq 0$, use the result of Part (d) to find $B$ such that $B x = y$ and $\\left\\| B\\right\\|  \\left\\| x\\right\\| =\\left\\| y\\right\\|$.  Then,\n",
    "$$\n",
    "\\left\\| A B x \\right\\| = \\left\\| A y \\right\\| = \\left\\| A \\right\\| \\left\\| y \\right\\| = \\left\\| A\\right\\| \\left\\| B\\right\\| \\left\\| x\\right\\|,\n",
    "$$\n",
    "so we have found a matrix $B$ that satisfies the condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Problem.  Accurate evaluation of an infinite series [2 pts]\n",
    "\n",
    "Consider an infinite series $s = \\sum_{k=1}^{\\infty} \\frac{1}{k^4}$.  The goal of this problem is to find the approximate value $\\hat{s}$ of $s$ with double-precision accuracy (i.e., $\\hat{s} = \\mathrm{fl}(s)$).\n",
    "\n",
    "The terms in the series decrease quickly to 0.  Therefore, the truncated series $s_n = \\sum_{k=1}^{n} \\frac{1}{k^4}$ would be a good approximation of the infinite series $s$ for sufficiently large $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) [1 pt] Find $n$ for which $s_n$ is a double-precision-or-more accurate approximation of $s$.  Use the following hints:\n",
    "- The error is $s - s_n = \\sum_{k=n+1}^{\\infty} \\frac{1}{k^4}$.  How small must this error be to achieve the desired accuracy in $s_n$?\n",
    "- What is a close upper bound of the error $s - s_n$?  Can you find one as the area under some curve, which can be calculated by a definite integral?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution.***\n",
    "\n",
    "Fon $s_n$ to be a double-precision-or-more accurate approximation of $s$, $\\abs{s - s_n}$ needs to be less than $\\epsmach=\\mathrm{eps()}/2$.\n",
    "\n",
    "From the following figure, we have \n",
    "$$\n",
    "s - s_n = \\sum_{k=n+1}^{\\infty} \\frac{1}{k^4}<\\int_{n}^{\\infty}\\frac{1}{x^4} dx = \\frac{1}{3n^3}.\n",
    "$$\n",
    "\n",
    "Therefore, to guarantee $s - s_n < \\epsmach$, we need to have $1/3n^3 < \\epsmach$, or $n > 1/(3 \\epsmach)^{1/3}=144263.40\\cdots$ equivalently.  Because $n$ is an integer, we conclude that $n$ needs to be at least $144264$.\n",
    "\n",
    "Of course, $1/3n^3$ in the above argument is a loose upper bound of $s-s_n$, so $s-s_n < \\epsmach$ can be achieved for smaller $n$.  However, $n$ different from $144264$ will be accepted as a valid solution only if it is proved that $s - s_n < \\epsmach$ for such $n$.\n",
    "\n",
    "<img src=\"x⁻⁴.pdf\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) [1 pt] In fact, $s = \\frac{\\pi^4}{90}$ exactly (see [here](https://en.wikipedia.org/wiki/Riemann_zeta_function#Specific_values)), so the desired $\\hat{s}$ is nothing but the floating-point representation of $\\frac{\\pi^4}{90}$. \n",
    "\n",
    "Now, the function `s_truncated` below takes `n` as an argument and evaluates the truncated series $s_n$.  You may expect that `s_truncated` would return $\\hat{s}$ if $n$ obtained in Part (a) is used as the argument, but you can verify that is not the case using the subsequent code block.\n",
    "\n",
    "Change one line of `s_truncated` to make it evaluate $s_n$ more accurately.  Briefly explain in words why your change makes `s_truncated` more accurate.  (Hint.  Which one is more accurate: `(2 + eps()) + eps()` or `2 + (eps() + eps())`?)\n",
    "\n",
    "After this change, you will see that `s_truncated` for $n$ obtained in Part (a) returns $\\hat{s}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution.***\n",
    "\n",
    "When two positive double-precision floating-point numbers $x_1 = m_1 \\beta^{e_1}$ and $x_2 = m_2 \\beta^{e_2}$ with $e_1 > e_2$ are added, they are first added in infinite precision as\n",
    "\n",
    "$$\n",
    "x_1 + x_2 = (m_1 + m_2 \\beta^{e_2-e_1}) \\beta^{e_1},\n",
    "$$\n",
    "\n",
    "and the result is approximated by a floating point number.  During this approximation, only the leading 53 bits of the mantissa are kept.  Therefore, many least-significant mantissa bits in $m_2$ are lost.  The greater the difference between $e_1$ and $e_2$ is, the more the mantissa bits are lost.  \n",
    "\n",
    "If we evaluate $\\sum_{k=1}^n 1/k^4$ in the obvious order, for some $m < n$ the partial sum $s_m = \\sum_{k=1}^m 1/k^4$ becomes much greater than the next term $1/(m+1)^4$ to add, and therefore many least-significant mantissa bits in $1/(m+1)^4$ are lost during addition.  Though these lost mantissa bits are the least significant bits and therefore constitute a small amount, such loss occurs for all the $\\frac{1}{k^4}$ terms for $k > m$ and is accumulated, leading to a noticeable difference in the final sum.\n",
    "\n",
    "Therefore, to avoid losing the least-significant mantissa bits of the addends, it is important to keep the partial sum as small as possible during summation (so that it is not too much greater than the next term to add), which is achieved by adding the smallest terms first.  For $s_n = \\sum_{k=1}^n 1/k^4$, this means we need to add the terms in the reverse order as in the following modified `s_trucated`.\n",
    "\n",
    "Note that this technique only works when we know the number of terms to add in advance: we cannot reverse the order of addition in the infinite sum $s = \\sum_{k=1}^{\\infty} 1/k^4$ because it does not have the last term.  This is why we needed to estimate $n$ in Part (a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s_truncated (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function s_truncated(n)\n",
    "    s = 0.0\n",
    "    for k = n:-1:1\n",
    "        s += 1.0/float(k)^4\n",
    "    end\n",
    "    \n",
    "    return s    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = ceil(1/(3eps()/2)^(1/3))  # use your n obtained in Part (a)\n",
    "ŝₙ = s_truncated(n)\n",
    "ŝ = π^4 / 90\n",
    "ŝ - ŝₙ"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.6.3-pre",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
