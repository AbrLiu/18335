#LyX 1.5.5 created this file. For more info see http://www.lyx.org/
\lyxformat 276
\begin_document
\begin_header
\textclass article
\begin_preamble

\renewcommand{\vec}[1]{\mathbf{#1}}

\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\roman{enumii})}

\newcommand{\tr}{\operatorname{tr}}
\end_preamble
\language english
\inputencoding auto
\font_roman times
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\paperfontsize default
\spacing single
\papersize default
\use_geometry true
\use_amsmath 2
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\topmargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Section*
18.335 Problem Set 6
\end_layout

\begin_layout Standard
Due Monday, 30 November.
\end_layout

\begin_layout Subsection*
Problem 1: Convexity
\end_layout

\begin_layout Standard
Recall from class that a 
\emph on
convex function
\emph default
 is a function 
\begin_inset Formula $f(x)$
\end_inset

 such that 
\begin_inset Formula $f(\alpha x+(1-\alpha)y)\leq\alpha f(x)+(1-\alpha)f(y)$
\end_inset

, for 
\begin_inset Formula $\alpha\in[0,1]$
\end_inset

, where 
\begin_inset Formula $x\in\mathbb{R}^{n}$
\end_inset

.
 The inequality is 
\begin_inset Formula $\geq$
\end_inset

 for 
\emph on
concave
\emph default
 functions, and = for 
\emph on
affine
\emph default
 functions.
 A 
\emph on
convex set 
\begin_inset Formula $X\subseteq\mathbb{R}^{n}$
\end_inset


\emph default
 satisfies the property that if 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 are in 
\begin_inset Formula $X$
\end_inset

, then so is the line connecting 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

, i.e.
 so is 
\begin_inset Formula $\alpha x+(1-\alpha)y$
\end_inset

 for 
\begin_inset Formula $\alpha\in[0,1]$
\end_inset

.
\end_layout

\begin_layout Standard
Show that the feasible region satisfying 
\begin_inset Formula $m$
\end_inset

 constraints 
\begin_inset Formula $f_{i}(x)\leq0$
\end_inset

, 
\begin_inset Formula $i=1,\ldots,m$
\end_inset

, is a convex set if the constraint functions are convex.
\end_layout

\begin_layout Subsection*
Problem 2: Adjoints
\end_layout

\begin_layout Standard
Consider a recurrence relation 
\begin_inset Formula $x^{n}=f(x^{n-1},p,n)\in\mathbb{R}^{K}$
\end_inset

 with initial condition 
\begin_inset Formula $x^{0}=b(p)$
\end_inset

 and 
\begin_inset Formula $P$
\end_inset

 parameters 
\begin_inset Formula $p\in\mathbb{R}^{P}$
\end_inset

, as in the notes from class (see the handout on the web page).
 In class, and in the handout, we used the adjoint method to derive an expressio
n for the derivatives 
\begin_inset Formula $\frac{dg^{N}}{dx}$
\end_inset

 of a function 
\begin_inset Formula $g(x^{n},p,n)\triangleq g^{n}$
\end_inset

 evaluated after 
\begin_inset Formula $N$
\end_inset

 steps.
\end_layout

\begin_layout Standard
In this problem, suppose that instead we want the derivative of a function
 
\begin_inset Formula $G$
\end_inset

 that depends on the values of 
\begin_inset Formula $x^{n}$
\end_inset

 from 
\emph on
every
\emph default
 
\begin_inset Formula $n\in\{0,1,\ldots,N\}$
\end_inset

 as follows: 
\begin_inset Formula \[
G(p,N)=\sum_{n=0}^{N}g(x^{n},p,n)\]

\end_inset

for some function 
\begin_inset Formula $g$
\end_inset

.
\end_layout

\begin_layout Enumerate
One could simply use the adjoint formula from class to obtain 
\begin_inset Formula $\frac{dG}{dp}=\sum_{n}\frac{dg^{n}}{dp}$
\end_inset

.
 Explain why this is inefficient.
\end_layout

\begin_layout Enumerate
Describe an efficient adjoint method to compute 
\begin_inset Formula $\frac{dG}{dp}$
\end_inset

.
 (Hint: modify the recurrence relation for 
\begin_inset Formula $\lambda^{n}$
\end_inset

 from class to compute 
\begin_inset Formula $\sum_{n}\frac{dg^{n}}{dp}$
\end_inset

 via the results of a 
\emph on
single
\emph default
 recurrence.)
\end_layout

\begin_layout Enumerate
Apply this to the example 
\begin_inset Formula $2\times2$
\end_inset

 recurrence and 
\begin_inset Formula $g$
\end_inset

 function from the notes, and implement your adjoint method in Matlab.
 Check your derivative 
\begin_inset Formula $\frac{dG}{dp}$
\end_inset

 with 
\begin_inset Formula $N=5$
\end_inset

 against the center-difference approximation 
\begin_inset Formula $\frac{dG}{dp_{i}}=[G(p_{i}+\delta)-G(p_{i}-\delta)]/2\delta$
\end_inset

 for 
\begin_inset Formula $p=(1,2,3,4,5)^{T}$
\end_inset

 and 
\begin_inset Formula $\delta p=10^{-5}$
\end_inset

.
\end_layout

\begin_layout Subsection*
Problem 3: Nonlinear fitting
\end_layout

\begin_layout Standard
One well-known application of nonlinear optimization is the problem of nonlinear
 fitting, also called nonlinear regression: you have a bunch of data points
 
\begin_inset Formula $(x_{i},y_{i})$
\end_inset

, and you want to fit them to a function 
\begin_inset Formula $f(x)$
\end_inset

 with some unknown parameters, where the parameters enter the function in
 a nonlinear way.
 A common problem is to find a 
\emph on
least-squares fit
\emph default
, the fit parameters minimizing the sum-of-squares error 
\begin_inset Formula $\sum_{i}[f(x_{i})-y_{i}]^{2}$
\end_inset

.
 Here, you'll do nonlinear fitting using the NLopt library, via Matlab.
 It is installed on Athena (x86 Linux machines only)---do 
\begin_inset Quotes eld
\end_inset

add stevenj
\begin_inset Quotes erd
\end_inset

 at the Athena prompt, and then, in Matlab, type 
\family typewriter
path(path,'/mit/stevenj/Public/nlopt_i386_deb50/matlab')
\family default
 to tell Matlab where to find NLopt.
\end_layout

\begin_layout Standard
In this problem you will fit a set of data points to a Lorentzian line shape
 (which often arises in resonant processes, e.g.
 looking at absorption lines in spectroscopy, or in NMR experiments).
 (There are actually much more sophisticated and robust techniques for the
 specific problem of fitting Lorentzian peaks, but we won't use them here.)
 That is, we have a bunch of data points 
\begin_inset Formula $(x_{i},y_{i})$
\end_inset

 that we want to fit to a curve of the form: 
\begin_inset Formula \[
f(x)=\frac{A}{(x-\omega)^{2}+\Gamma^{2}}\]

\end_inset

in terms of some unknown 
\begin_inset Quotes eld
\end_inset

resonant frequency
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\omega$
\end_inset

, 
\begin_inset Quotes eld
\end_inset

lifetime
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $\Gamma$
\end_inset

, and amplitude 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Standard
The file lorentzdata.m on the course page contains a function [x,y] = lorentzdata
(
\begin_inset Formula $N$
\end_inset

, 
\begin_inset Formula $A$
\end_inset

, 
\begin_inset Formula $\omega$
\end_inset

, 
\begin_inset Formula $\Gamma$
\end_inset

, 
\emph on
noise
\emph default
) that generates 
\begin_inset Formula $N$
\end_inset

 random 
\begin_inset Quotes eld
\end_inset

data
\begin_inset Quotes erd
\end_inset

 points based on the Lorentzian peak with paramters 
\begin_inset Formula $A$
\end_inset

, 
\begin_inset Formula $\omega$
\end_inset

, and 
\begin_inset Formula $\Gamma$
\end_inset

, with some random noise of amplitude 
\emph on
noise
\emph default
 added in.
 The file lorentzfit.m computes the sum-of-squares error given a vector p
 = [
\begin_inset Formula $A,\omega,\Gamma$
\end_inset

] of the fit parameters and the point arrays x and y.
\end_layout

\begin_layout Enumerate
Try fitting 200 random data points from 
\begin_inset Formula $A=1$
\end_inset

, 
\begin_inset Formula $\omega=0$
\end_inset

, and 
\begin_inset Formula $\Gamma=1$
\end_inset

, with noise 
\begin_inset Formula $\pm0.1$
\end_inset

, to minimize the sum-of-squares error, using:
\begin_inset Include \verbatiminput{pset6p2a.m}
preview false

\end_inset

This is calling NLopt to do the minimization with a derivative-free algorithm
 called NEWUOA that constructs approximate quadratic models of the objective
 function (lorentzfit).
 Note the [-inf,-inf,0], [inf,inf,inf] arguments, which give lower and upper
 bounds for the parameters 
\begin_inset Formula $[A,\omega,\Gamma$
\end_inset

] (which are unconstrained except that we require 
\begin_inset Formula $\Gamma\geq0$
\end_inset

).
 The last line gives an initial 
\begin_inset Quotes eld
\end_inset

guess
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $A=0$
\end_inset

, 
\begin_inset Formula $\omega=1$
\end_inset

, 
\begin_inset Formula $\Gamma=2$
\end_inset

.
 The optimization terminates when an estimated fractional error 
\begin_inset Formula $10^{-8}$
\end_inset

 is reached on the parameters.
 Plot the fit curve, which is returned in the parameters p0, and the data
 points to verify that the fit looks reasonable.
 Do the fit parameters vary significantly depending on the initial guess?
\end_layout

\begin_layout Enumerate
Because of the line 
\begin_inset Quotes eld
\end_inset

stop.verbose=1
\begin_inset Quotes erd
\end_inset

, you can see how many function evaluations are required by the optimization
 algorithm to converge.
 Change stop.xtol_rel to zero (no tolerance), and set stop.minf_max = errmin0
 * 1.0001.
 This will stop the optimization when the objective function gets within
 
\begin_inset Formula $10^{-4}$
\end_inset

 of the previous value (errmin0), which allows us to compare different algorithm
s easily---we can see how many iterations are required to reach the same
 value of the objective function.
 With this new stopping criteria, compare the number of iterations for NEWUOA
 (NLOPT_LN_NEWUOA_BOUND) with a different algorithm that constructs only
 linear approximations of the objective function (NLOPT_LN_COBYLA), with
 the Nelder-Mead simplex algorithm (NLOPT_LN_NELDERMEAD).
\end_layout

\begin_layout Enumerate
All of the previous algorithms were derivative-free (they don't use the
 gradient of the objective, only its values).
 However lorentzfit also optionally returns the gradient, and we can use
 that.
 With the same stopping criterion as in the previous part, try the gradient-base
d algorithms NLOPT_LD_MMA and NLOPT_LD_LBFGS.
 LBFGS iteratively constructs quadratic approximations that try to match
 the first and second derivative (similar in spirit to NEWUOA), while MMA
 uses only the first derivative; what is the impact of this on the rate
 of convergence?
\end_layout

\begin_layout Enumerate
Now, modify lorentzdata and lorentzfit so that the data is the sum of 
\emph on
two
\emph default
 overlapping Lorentzians: 
\begin_inset Formula $A=1$
\end_inset

, 
\begin_inset Formula $\omega=0$
\end_inset

, and 
\begin_inset Formula $\Gamma=1$
\end_inset

, and 
\begin_inset Formula $A=2$
\end_inset

, 
\begin_inset Formula $\omega=1$
\end_inset

, and 
\begin_inset Formula $\Gamma=1.5$
\end_inset

, with noise = 0.01.
 Try fitting again with any of the algorithms from above, changing the stopping
 criteria to stop.xtol_rel=1e-4 and stop.minf_max=-inf.
 Try different starting points; do you get more than one local minimum?
 How close does your initial guess need to be to the 
\begin_inset Quotes eld
\end_inset

correct
\begin_inset Quotes erd
\end_inset

 answer in order to recover something like the parameters you used to generate
 the data?
\end_layout

\begin_layout Enumerate
Try the previous part, but use one of the 
\emph on
global
\emph default
 optimization algorithms, like NLOPT_GN_DIRECT_L or NLOPT_GD_MLSL_LDS, or
 NLOPT_GN_CRS2_LM (see the NLopt manual, at 
\family typewriter
ab-initio.mit.edu/nlopt
\family default
, for what these algorithms are).
 You'll need to specify finite search-box constraints; use [0,0,0,0,0,0]
 for the lower bounds and [5,5,5,5,5,5] for the upper bounds.
 You'll also need to change the stopping conditions.
 Change stop.xtol_rel = 0 and and just vary the maximum number of function
 evaluations---set stop.maxeval = 1000 to start with (at most 1000 function
 evaluations).
 How big does the number of function evaluations need to be to recover (roughly)
 the parameters you used to generate the data? Is there a different global
 optimization algorithm in NLopt that requires significantly fewer evaluations
 for similar accuracy?
\end_layout

\begin_layout Enumerate
Most global-optimization methods spend most of their time searching the
 parameter space and not much time 
\begin_inset Quotes eld
\end_inset

polishing
\begin_inset Quotes erd
\end_inset

 the local optima, so it is better to finish them off by running a local
 optimization at the end.
 If you do this for DIRECT_L, running LBFGS at the end to get the final
 local optimum more accurately, can you significantly reduce the maxeval
 from the global search while still recovering the original parameters accuratel
y?
\end_layout

\end_body
\end_document
